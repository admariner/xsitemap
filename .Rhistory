" web page url(s) found"))
return(NULL)
}
urls <- data.frame(
loc = character(),
lastmod = as.Date(character(), format = "%Y-%m-%d"),
stringsAsFactors = FALSE
)
for (i in 1:nb_children) {
individual_sitemap <-  xml_data[i]$sitemap$loc
if (!is.null(individual_sitemap)) {
message(paste0("\n", i, " >>> ", individual_sitemap))
new_urls <- xsitemapGet(individual_sitemap)
parsed_urls <- urltools::url_parse(individual_sitemap)
if(!is.na(parsed_urls$parameter)){
new_urls$origin <-
paste0(parsed_urls$path,"?",parsed_urls$parameter)
}else{
new_urls$origin <-
parsed_urls$path
}
urls <- rbind(urls, new_urls)
}
return(urls)
} else{
# if (nb_children < 50000) {
message(paste(
"regular sitemap detected - ",
nb_children,
" web page url(s) found"
))
#} else{
#  warning(paste("too many URLs - ",
#                nb_children,
#                " web page url(s) found"))
#  return(NULL)
#urls <- vector(mode = "character", length = 0)
urls <- data.frame(
loc = character(),
lastmod = as.Date(character(), format = "%Y-%m-%d"),
stringsAsFactors = FALSE
)
for (i in 1:(nb_children - 1)) {
cat(".")
urls[i, ]$loc <- xml_data[i]$url$loc
if (!is.null(xml_data[i]$url$lastmod)) {
urls[i, ]$lastmod <- xml_data[i]$url$lastmod
}
return(urls)
}
} else{
stop("Mal formatted url")
#return NA
}
test <- xsitemapGet("https://squireme.com/sitemap.xml")
View(test)
test <- xsitemapGet("https://squireme.com/sitemap.xml")
test <- xsitemapGet("https://squireme.com/")
test <- xsitemapGet("https://www.gokam.fr/")
library(devtools)
install_github("pixgarden/xsitemap")
test <- xsitemapGet("https://www.gokam.fr/")
library(xsitemap)
test <- xsitemapGet("https://www.gokam.fr/")
View(test)
test <- xsitemapGet("https://graemewinchester.co.uk/")
test2 <- xsitemapCheckHTTP(test)
install.packages("progress")
library(progress)
pb <- progress_bar$new(total = 100)
for (i in 1:100) {
pb$tick()
Sys.sleep(1 / 100)
}
pb <- progress_bar$new(total = 200)
for (i in 1:100) {
pb$tick()
Sys.sleep(1 / 200)
}
pb <- progress_bar$new(total = 200)
for (i in 1:2100) {
pb$tick()
Sys.sleep(1 / 200)
}
pb <- progress_bar$new(total = 200)
for (i in 1:200) {
pb$tick()
Sys.sleep(1 / 200)
}
?xsitemapCheckHTTP()
devtools::load_all().
devtools::load_all()
devtools::use_testthat()
devtools::test()
test <- xsitemapGet("https://www.gov.uk/")
View(test)
test <- xsitemapGet("https://cran.r-project.org/")
test <- xsitemapGet("https://www.r-project.org/")
test
xsitemapCheckHTTP("https://www.r-project.org/")
xsitemapGuess("https://www.r-project.org/")
test <- xsitemapGuess("https://www.r-project.org/")
test
devtools::test().
devtools::test()
test_check("xsitemap")
devtools::test()
test <- xsitemapGuess("https://hackney.gov.uk/")
View(test)
test <- xsitemapGet("https://hackney.gov.uk/")
test <- xsitemapGuess("https://www.greenpeace.fr/")
test <- xsitemapGet("https://www.greenpeace.fr/")
test <- xsitemapGet("https://www.booking.com/")
test <- xsitemapGet("https://www.booking.com/sitembk-reviews-index-hotel-review.xml")
View(test)
test <- xsitemapGet("https://builtwith.com/")
View(test)
test <- request("http://www.booking.com/sitembk-district.no.0000.xml.gz")
url <- http://www.booking.com/sitembk-district.ko.0000.xml.gz
url <- "http://www.booking.com/sitembk-district.ko.0000.xml.gz"
gsub("\\*$", "", url)
gsub("\\gz$", "", url)
gsub("\\.gz$", "", url)
gsub("\\\.gz$", "", url)
gsub("\\.gz$", "", url)
url <- "http://www.booking.com/sitembk-district.ko.0000.xml?gz"
gsub("\\.gz$", "", url)
url <- "http://www.booking.com/sitembk-district.ko.0000.xml.gz"
gsub("\\.gz$", "", url)
user_agent
user_agent <-
"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36"
request <- GET(url, user_agent(user_agent))
request
untar(untar)
untar(request)
untar(request$content)
file <- download.file("http://www.booking.com/sitembk-district.ko.0000.xml.gz")
tmpdir <- tempdir()
url <- 'http://www.booking.com/sitembk-district.ko.0000.xml.gz'
file <- basename(url)
download.file(url, file)
untar(file, compressed = 'gzip', exdir = tmpdir )
list.files(tmpdir)
tmpdir <- tempdir()
url <- 'http://www.booking.com/sitembk-district.ko.0000.xml.gz'
file <- basename(url)
download.file(url, file)
untar(file, exdir = tmpdir )
untar(file, compressed = 'gz', exdir = tmpdir )
file
untar(file, compressed = 'gzip', exdir = tmpdir )
?GET
request <- GET(url, user_agent(user_agent))
set_config(verbose())
request <- GET(url, user_agent(user_agent))
ls(httr:::parsers)
scan(gzcon(rawConnection(content(GET("http://glimmer.rstudio.com/alexbbrown/gz/sample.txt.gz")))),"",,,"\n")
assign("application/x-gzip", function(x, ...) {
f <- tempfile();
writeBin(x,f);
if(!is.null(list(...)$list)){
if(list(...)$list){
return(untar(f, list = TRUE))
}else{
untar(f, ...);
readLines(f)
}
}else{
untar(f, ...);
readLines(f)
}
}, envir = httr:::parsers)
content(GET("http://cran.r-project.org/src/contrib/httr_0.2.tar.gz"), as = "parsed", list = TRUE)
content(GET("http://www.booking.com/sitembk-district.ko.0000.xml.gz"), as = "parsed", list = TRUE)
content(GET("https://www.booking.com/sitembk-district.ko.0000.xml.gz"), as = "parsed", list = TRUE)
request
result <- content(req, type = "application/json; charset=utf-8")
result <- content(request, type = "application/json; charset=utf-8")
result <- content(request, type = "text/plain; charset=utf-8")
result
View(result)
result <- content(request$content, type = "text/plain; charset=utf-8")
request$status_code
content(request, type = "text/xml; charset=utf-8")
content(request, type = "text/xml")
?content
test <- GET("https://www.booking.com/sitembk-district.ko.0000.xml.gz",
add_headers(`Accept-Encoding` = "gzip, deflate"))
test$content
result <- content(request$content, type = "text/plain; charset=utf-8")
test <- GET("https://www.booking.com/sitembk-district.ko.0000.xml.gz",
add_headers(`Accept-Encoding` = "None"))
test$content
result <- content(request$content, type = "text/plain; charset=utf-8")
result <- content(request$content, type = "xml/text; charset=utf-8")
result <- content(request, type = "xml/text; charset=utf-8")
View(result)
untar(request$content)
?untar(request$content,)
untar(request$content,test)
untar(request$content,"./test.xml")
test <- GET("https://www.booking.com/sitembk-district.ko.0000.xml.gz",
add_headers(`Accept-Encoding` = "identity"))
test$content
res1 = GET(url = "https://www.booking.com/sitembk-district.ko.0000.xml.gz",
add_headers(`Accept-Encoding` = "gzip, deflate"))
res2 = GET(url = "https://www.booking.com/sitembk-district.ko.0000.xml.gz")
res1 == res2
res1$url == res2$url
res1$content == res2$content
summary(res1$content == res2$content)
res1$headers
res1$content
class(res1$content)
content(res1)
content(res1$content)
content(res1)
content_type(res1)
?content_type()
request
request$content
conte(content(request))
content(request)
content(res1)
r <- POST("http://httpbin.org/post", body = list(a = 1, b = 2))
content(r) # automatically parses JSON
cat(content(r, "text"), "\n") # text content
content(r, "raw") # raw bytes from server
content(res1)
content(res2)
content(res2,"raw")
content(res1,as="text")
content(res1$content,as="text")
res1$content
res1 = GET(url = "https://www.booking.com/sitembk-district.ko.0000.xml.gz",
add_headers(`Accept-Encoding` = "gzip, deflate"))
res1 = GET(url = "https://www.booking.com/sitembk-district.ko.0000.xml.gz")
res1$content
unzip(res1)
unzip(res1$content)
dest <- paste0(getwd(), "/data/weekly_2017-02-18.zip")
GET("https://www.booking.com/sitembk-district.ko.0000.xml.gz", write_disk(dest, overwrite = TRUE))
getwd()
rawToChar(res1$content)
cat(rawToChar(res1$content))
content(res1, "parse")
rest1$content
res1$content
rawToBits(res1$content)
rawToChar(res1$content)
untar(res1,compressed="gzip")
untar(res1$content,compressed="gzip")
assign("application/octet-stream", function(x, ...) {f <- tempfile(); writeBin(x,f);untar(f);readLines(f, warn = FALSE)},envir = httr:::parsers)
content(GET("http://glimmer.rstudio.com/alexbbrown/gz/sample.txt.gz"), as = "parsed")
content(GET("https://www.booking.com/sitembk-district.ko.0000.xml.gz"), as = "parsed")
?unzip()
unzip(res1)
unzip(res1$content)
unzip(files = res1$content)
test <- xsitemapGet("http://www.nationalarchives.gov.uk/")
test <- xsitemapGetFromRobotsTxt("http://www.nationalarchives.gov.uk/")
urltocheck <- "http://www.nationalarchives.gov.uk/"
user_agent <-
"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36"
if ("/robots.txt" != substr(urltocheck, nchar(urltocheck) - 10, nchar(urltocheck))) {
if ("/" != substr(urltocheck, nchar(urltocheck),  nchar(urltocheck))) {
urltocheck <- paste0(urltocheck, "/robots.txt")
} else{
urltocheck <- paste0(urltocheck, "robots.txt")
}
urltocheck
request <- httr::GET(urltocheck, user_agent(user_agent))
request$status_code
robotstext <- httr::content(request)
robotstext
stringr::str_detect(robotstext, "\nSitemap:")
stringr::str_match(robotstext, "Sitemap: (.*)(\\n|$)")[, 2]
stringr::str_match(robotstext, "Sitemap:(.*)(\\n|$)")[, 2]
stringr::str_match(robotstext, "(.*)Sitemap:(.*)(\\n|$)")[, 2]
stringr::str_match(robotstext, "(.*)Sitemap:(.*)(\\n|$)")
stringr::str_match(robotstext, "(.*)Sitemap:(.*)(\\n|$|\\r)")
stringr::str_match(robotstext, "(.*)Sitemap:(.*)(\\n|$|\\r)")[, 2]
stringr::str_match(robotstext, "(.*)Sitemap:(.*)(\\n|$|\\r)")[, 1]
stringr::str_match(robotstext, "(.*)Sitemap:A-Za-z0-9_.~!*''();:@&=+$,/?#[%-]+(\\n|$|\\r)")[, 1]
stringr::str_match(robotstext, "(.*)Sitemap:A-Za-z0-9_.~!*''();:@&=+$,/?#[%-]+(\\n|$|\\r)")
stringr::str_match(robotstext, "Sitemap: (.*)(\\n|$|\\r)")[, 2]
robotstext
stringr::str_match(robotstext, "Sitemap: (.*)(\\n|$|\\r)")[, 2]
askYesNo("msg", default = TRUE,
prompts = getOption("askYesNo", gettext(c("Yes", "No", "Cancel"))),
...)
askYesNo("msg", default = TRUE,
prompts = getOption("askYesNo", gettext(c("Yes", "No", "Cancel"))))
askYesNo("Are you sure ?", default = TRUE,
prompts = getOption("askYesNo", gettext(c("Yes", "No"))))
askYesNo("msg", default = TRUE,
prompts = getOption("askYesNo", gettext(c("Yes", "No", "Cancel"))))
askYesNo("msg", default = TRUE,
prompts = getOption("askYesNo", gettext(c("Yes", "No"))))
askYesNo("msg", default = TRUE,
prompts = getOption("askYesNo", gettext(c("Yes", "No", "Cancel"))))
askYesNo(paste("msg",3), default = TRUE,
prompts = getOption("askYesNo", gettext(c("Yes", "No", "Cancel"))))
paste("Are you sure you want to crawl", nrow(sitemap), "URLs"),
default = TRUE,
prompts = getOption("askYesNo", gettext(c(
"Yes", "No", "Cancel"
)))
askYesNo(
paste("Are you sure you want to crawl", nrow(sitemap), "URLs"),
default = TRUE,
prompts = getOption("askYesNo", gettext(c(
"Yes", "No", "Cancel"
))))
#' xsitemapCheckHTTP
#'
#' Check if xml sitemap urls send a 200 http code
#'
#' @param sitemap dataframe
#'
#' @return dataframe
#' @export
#'
xsitemapCheckHTTP <- function(sitemap) {
message(paste("xsitemapCheckHTTP :", nrow(sitemap), " URL(s) to check"))
if (nrow(sitemap) < 1000) {
if (!askYesNo(
paste("Are you sure you want to crawl", nrow(sitemap), "URLs"),
default = TRUE,
prompts = getOption("askYesNo", gettext(c(
"Yes", "No", "Cancel"
)))
)) {
return(NULL)
}
user_agent <-
"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36"
sitemap["http"] <- NA
for (i in 1:nrow(sitemap)) {
#progress.bar = TRUE
cat(".")
request <-
HEAD(sitemap[i,]$loc,
user_agent(user_agent),
config(followlocation = 0))
sitemap[i,]$http <- request$status_code
}
return(sitemap)
}
test <- xsitemapGet("http://www.nationalarchives.gov.uk")
devtools::test()
xsitemapGet("https://www.r-project.org/")
xsitemapGet("https://cran.r-project.org/")
xsitemapGet("http://www.example.com")
devtools::test()
test_package("xsitemap")
devtools::test()
xsitemapGetFromRobotsTxt("https://www.sitemaps.org/")
user_agent <-
"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36"
urltocheck <- "https://www.sitemaps.org/"
if ("/robots.txt" != substr(urltocheck, nchar(urltocheck) - 10, nchar(urltocheck))) {
if ("/" != substr(urltocheck, nchar(urltocheck),  nchar(urltocheck))) {
urltocheck <- paste0(urltocheck, "/robots.txt")
} else{
urltocheck <- paste0(urltocheck, "robots.txt")
}
urltocheck
request <- httr::GET(urltocheck, user_agent(user_agent))
request$status_code
robotstext <- httr::content(request)
robotstext
stringr::str_detect(robotstext, "\nSitemap:")
stringr::str_detect(robotstext, "(\n|^)Sitemap:")
#' xsitemapGetFromRobotsTxt
#' ceci est un description tres partiel
#' @param urltocheck hostname string of the website you want to find xml sitemap from robots
#'
#' @return string
#' @export
#'
xsitemapGetFromRobotsTxt <- function(urltocheck) {
user_agent <-
"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36"
if ("/robots.txt" != substr(urltocheck, nchar(urltocheck) - 10, nchar(urltocheck))) {
if ("/" != substr(urltocheck, nchar(urltocheck),  nchar(urltocheck))) {
urltocheck <- paste0(urltocheck, "/robots.txt")
} else{
urltocheck <- paste0(urltocheck, "robots.txt")
}
request <- httr::GET(urltocheck, user_agent(user_agent))
if (request$status_code == 200) {
robotstext <- httr::content(request)
if (stringr::str_detect(robotstext, "(\n|^)Sitemap:")) {
message("XML sitemap url detect inside robots.txt")
#library(stringr)
xml_url <-
stringr::str_match(robotstext, "(\n|^)Sitemap: (.*)(\\n|$|\\r)")[, 2]
message(xml_url)
return(paste0("", xml_url))
} else{
message("No XML sitemap url inside robots.txt")
return("")
}
} else{
message("no robots.txt")
return("")
}
test_xsitemap_again <- xsitemapGet("https://www.sitemaps.org/")
stringr::str_detect(robotstext, "(\n|^)Sitemap:")
urltocheck
user_agent <-
"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36"
if ("/robots.txt" != substr(urltocheck, nchar(urltocheck) - 10, nchar(urltocheck))) {
if ("/" != substr(urltocheck, nchar(urltocheck),  nchar(urltocheck))) {
urltocheck <- paste0(urltocheck, "/robots.txt")
} else{
urltocheck <- paste0(urltocheck, "robots.txt")
}
request <- httr::GET(urltocheck, user_agent(user_agent))
if (request$status_code == 200) {
robotstext <- httr::content(request)
if (stringr::str_detect(robotstext, "(\n|^)Sitemap:")) {
message("XML sitemap url detect inside robots.txt")
#library(stringr)
xml_url <-
stringr::str_match(robotstext, "(\n|^)Sitemap: (.*)(\\n|$|\\r)")[, 2]
message(xml_url)
return(paste0("", xml_url))
} else{
message("No XML sitemap url inside robots.txt")
return("")
}
} else{
message("no robots.txt")
return("")
}
#library(stringr)
xml_url <-
stringr::str_match(robotstext, "(\n|^)Sitemap: (.*)(\\n|$|\\r)")[, 2]
message(xml_url)
View(res2)
message(xml_url)
stringr::str_match(robotstext, "(\n|^)Sitemap: (.*)(\\n|$|\\r)")[, 2]
stringr::str_match(robotstext, "(\n|^)Sitemap: (.*)(\\n|$|\\r)")
stringr::str_match(robotstext, "(?\n|^)Sitemap: (.*)(\\n|$|\\r)")
stringr::str_match(robotstext, "(\n|^)Sitemap: (.*)(\\n|$|\\r)")
stringr::str_match(robotstext, "(\n|^)Sitemap: (.*)(\\n|$|\\r)")[,3]
#' xsitemapGetFromRobotsTxt
#' ceci est un description tres partiel
#' @param urltocheck hostname string of the website you want to find xml sitemap from robots
#'
#' @return string
#' @export
#'
xsitemapGetFromRobotsTxt <- function(urltocheck) {
user_agent <-
"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36"
if ("/robots.txt" != substr(urltocheck, nchar(urltocheck) - 10, nchar(urltocheck))) {
if ("/" != substr(urltocheck, nchar(urltocheck),  nchar(urltocheck))) {
urltocheck <- paste0(urltocheck, "/robots.txt")
} else{
urltocheck <- paste0(urltocheck, "robots.txt")
}
request <- httr::GET(urltocheck, user_agent(user_agent))
if (request$status_code == 200) {
robotstext <- httr::content(request)
if (stringr::str_detect(robotstext, "(\n|^)Sitemap:")) {
message("XML sitemap url detect inside robots.txt")
#library(stringr)
xml_url <-
stringr::str_match(robotstext, "(\n|^)Sitemap: (.*)(\\n|$|\\r)")[, 3]
message(xml_url)
return(paste0("", xml_url))
} else{
message("No XML sitemap url inside robots.txt")
return("")
}
} else{
message("no robots.txt")
return("")
}
xsitemapGetFromRobotsTxt("https://www.sitemaps.org/")
install.packages("RAdwords")
library(RAdwords)
google_auth <- doAuth()
library(RAdwords)
?RAdwords
?xsitemap
Create Statement:
body <- statement(select = c('Clicks','AveragePosition','Cost','Ctr'),
report = "ACCOUNT_PERFORMANCE_REPORT",
start = "2018-01-01",
end = "2018-01-10")
Query Adwords API and load data as dataframe:
data <- getData(clientCustomerId = 'xxx-xxx-xxxx', #use Adwords Account Id (MCC Id will not work)
google_auth = google_auth,
statement = body)
Get available report types:
reports()
Get available metrics/attributes of specific report type:
metrics(report = 'ACCOUNT_PERFORMANCE_REPORT')
body <- statement(select = c('Clicks','AveragePosition','Cost','Ctr'),
report = "ACCOUNT_PERFORMANCE_REPORT",
start = "2018-01-01",
end = "2018-01-10")
data <- getData(clientCustomerId = 'xxx-xxx-xxxx', #use Adwords Account Id (MCC Id will not work)
google_auth = google_auth,
statement = body)
reports()
metrics(report = 'ACCOUNT_PERFORMANCE_REPORT')
